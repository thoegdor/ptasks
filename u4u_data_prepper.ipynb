{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The purpose of this workbook is to...\n",
    "\n",
    "...process all relevant files as needed to move into central local db.\n",
    "\n",
    "#### Overview:\n",
    "\n",
    "<p>1) Get tables cols into a good place for loading\n",
    "<p>2) Build a class that includes the following pieces of functionality:\n",
    "    -  Drop/Create Table(s)\n",
    "    -  Load Table(s) \n",
    "    -  Query + Preview Table(s) for Checking Purposes\n",
    "<p>3) Make use of above functionality\n",
    "\n",
    "<p>---\n",
    "    \n",
    "<p>The above should be flexible enough across tables. I'll start by working with at least two tables, maybe introducing more along the way...\n",
    "    -  311 Complaints - the main table that houses all complaint info\n",
    "    -  NYC ZIP codes - for any joins/bringing on additional geographic information\n",
    "    \n",
    "<p>In light of the above, the following are legitimate questions: \n",
    "    - Why build something this complex? Why not just read_csv everything later on?\n",
    "<p>There are different reasons, the main one being: dealing with a ~10GB file in memory without any pre-processing is daunting on a basic machine like mine, so it will help to do some db work first. Additionally, building out functionality to make changes to db tables quickly will save potential frustration down the road and open up new opportunities if we want to bring in new tables at any point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load packages\n",
    "import sqlite3, csv, re\n",
    "import pandas as pd\n",
    "\n",
    "# connect to db\n",
    "con = sqlite3.connect(\"techtask.db\")\n",
    "cur = con.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deal with all cols as needed\n",
    "\n",
    "# 311 data portal here: \n",
    "#https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9\n",
    "\n",
    "# pull complaints cols\n",
    "complaints_cols = ['Unique Key', 'Created Date', 'Closed Date', 'Agency', 'Agency Name',\n",
    "                   'Complaint Type', 'Descriptor', 'Location Type', 'Incident Zip',\n",
    "                   'Incident Address', 'Street Name', 'Cross Street 1', 'Cross Street 2',\n",
    "                   'Intersection Street 1', 'Intersection Street 2', 'Address Type',\n",
    "                   'City', 'Landmark', 'Facility Type', 'Status', 'Due Date',\n",
    "                   'Resolution Description', 'Resolution Action Updated Date',\n",
    "                   'Community Board', 'BBL', 'Borough', 'X Coordinate (State Plane)',\n",
    "                   'Y Coordinate (State Plane)', 'Open Data Channel Type',\n",
    "                   'Park Facility Name', 'Park Borough', 'Vehicle Type',\n",
    "                   'Taxi Company Borough', 'Taxi Pick Up Location', 'Bridge Highway Name',\n",
    "                   'Bridge Highway Direction', 'Road Ramp', 'Bridge Highway Segment',\n",
    "                   'Latitude', 'Longitude', 'Location']\n",
    "\n",
    "# make complaints cols more db friendly\n",
    "complaints_cols_dict = {}\n",
    "\n",
    "for c in complaints_cols:\n",
    "    \n",
    "    # sub out any special characters\n",
    "    c1 = re.sub(\"[^0-9A-Za-z\\\\s+]\", \"\", c)\n",
    "    complaints_cols_dict[c] = c1\n",
    "    \n",
    "    # join strings with underscores\n",
    "    c2 = \"_\".join(c1.split())\n",
    "    complaints_cols_dict[c] = c2\n",
    "    \n",
    "    # lower strings\n",
    "    c3 = c2.lower()\n",
    "    complaints_cols_dict[c] = c3\n",
    "    \n",
    "# nyc zips - something I culled together from here: \n",
    "# https://www.health.ny.gov/statistics/cancer/registry/appendix/neighborhoods.htm\n",
    "\n",
    "nyc_zips_cols = ['pk','borough','neighborhood','zip_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create all meta-data that the class will require\n",
    "# some vars will be created, then stuffed into a dict for convenience\n",
    "\n",
    "# table ddls - won't need these if we use pd.to_sql, but keeping for record's sake\n",
    "\n",
    "nyc_zips_ddl = \"(\\\n",
    "pk STR,\\\n",
    "borough STR,\\\n",
    "neighborhood STR,\\\n",
    "zip_code STR)\"\n",
    "\n",
    "complaints_ddl = \"( \\\n",
    "unique_key STR, \\\n",
    "created_date STR,\\\n",
    "closed_date STR,\\\n",
    "agency STR,\\\n",
    "agency_name STR, \\\n",
    "complaint_type STR,\\\n",
    "descriptor STR,\\\n",
    "location_type STR,\\\n",
    "incident_zip STR,\\\n",
    "incident_address STR,\\\n",
    "street_name STR,\\\n",
    "cross_street_1 STR,\\\n",
    "cross_street_2 STR,\\\n",
    "intersection_street_1 STR,\\\n",
    "intersection_street_2 STR,\\\n",
    "address_type STR,\\\n",
    "city STR,\\\n",
    "landmark STR,\\\n",
    "facility_type STR,\\\n",
    "status STR,\\\n",
    "due_date STR,\\\n",
    "resolution_description STR, \\\n",
    "resolution_action_updated_date STR,\\\n",
    "community_board STR,\\\n",
    "bbl STR,\\\n",
    "borough STR,\\\n",
    "x_coordinate_state_plane STR,\\\n",
    "y_coordinate_state_plane STR,\\\n",
    "open_data_channel_type STR,\\\n",
    "park_facility_name STR,\\\n",
    "park_borough STR,\\\n",
    "vehicle_type STR,\\\n",
    "taxi_company_borough STR,\\\n",
    "taxi_pick_up_location STR,\\\n",
    "bridge_highway_name STR,\\\n",
    "bridge_highway_direction STR,\\\n",
    "road_ramp STR,\\\n",
    "bridge_highway_segment STR,\\\n",
    "latitude STR,\\\n",
    "longitude STR,\\\n",
    "location STR)\" \n",
    "\n",
    "# load funcs \n",
    "\n",
    "def load_nyc_zips(file_name,table,load_cols,con):\n",
    "    \n",
    "    # load using pandas \n",
    "    df = pd.read_csv(file_name)\n",
    "    df.columns = load_cols\n",
    "    df.to_sql(name = table,\n",
    "              con = con,\n",
    "              if_exists='fail', # shouldn't already exist\n",
    "              index=False)\n",
    "    \n",
    "def load_complaints(file_name,table,load_cols,con):\n",
    "    \n",
    "    # load this large file by inserting into by chunks.\n",
    "    # this helps prevent any explosions\n",
    "    \n",
    "    chunksize = 100000\n",
    "\n",
    "    for df in pd.read_csv(file_name, \n",
    "                      chunksize=chunksize, \n",
    "                      iterator=True,\n",
    "                      low_memory=False):\n",
    "    \n",
    "        df = df.rename(columns=load_cols) \n",
    "        df.to_sql(table,con, if_exists='append',index=False)\n",
    "\n",
    "# store in central dict\n",
    "meta_dict = {\n",
    "    \n",
    "    \"nyc_zips\":{\"file_name\":\"nyc_zip_codes.csv\",\n",
    "                 \"ddl\":nyc_zips_ddl,\n",
    "                 \"load_cols\":nyc_zips_cols,\n",
    "                 \"load_func\":load_nyc_zips},\n",
    "    \n",
    "    \"complaints\":{\"file_name\":\"311_data.csv\",\n",
    "                  \"ddl\":complaints_ddl,\n",
    "                  \"load_cols\":complaints_cols_dict,\n",
    "                  \"load_func\":load_complaints}\n",
    "}\n",
    "\n",
    "# build a separate  db dict \n",
    "db_dict = {\"cur\":cur,\n",
    "           \"con\":con}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the class to store and run everything\n",
    "class dbManager:\n",
    "\n",
    "    def __init__(self,file_name,table,ddl,load_func,load_cols,db_dict):\n",
    "        self.file_name = file_name\n",
    "        self.table = table\n",
    "        self.ddl = ddl\n",
    "        self.load_func = load_func\n",
    "        self.load_cols = load_cols\n",
    "        self.db_dict = db_dict\n",
    "       \n",
    "    def drop(self):\n",
    "\n",
    "        # this originally was drop+create, but decided to use \n",
    "        # pd.to_sql for creating/loading. So just dropping here...\n",
    "        \n",
    "        print(\"dropping \" + self.table)\n",
    "        self.db_dict['cur'].execute(\"DROP TABLE \" + self.table + \";\")\n",
    "        self.db_dict['con'].commit()\n",
    "       \n",
    "    def create_and_load(self):\n",
    "       \n",
    "        print(\"creating and loading \" + self.table)\n",
    "        self.load_func(file_name = self.file_name,\n",
    "                       load_cols = self.load_cols,\n",
    "                       table = self.table,\n",
    "                       con = self.db_dict['con'])\n",
    "       \n",
    "    def check(self):\n",
    "        \n",
    "        just_table_row_count = str(pd.read_sql(\"SELECT COUNT(*) FROM \" + self.table + \";\",con).iloc[0][0])\n",
    "        print(\"row count of \" + self.table + \" = \" + just_table_row_count)\n",
    "        print(\"getting first 100 rows \" + self.table + \" from the db for you to preview...\")\n",
    "        print(pd.read_sql(\"SELECT * FROM \" + self.table + \" LIMIT 100;\",con))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the class to interact with db\n",
    "#try:\n",
    "#\n",
    "#    table = \"nyc_zips2\"\n",
    "#\n",
    "#    dbManager(\n",
    "#        file_name = meta_dict[table]['file_name'],\n",
    "#        table = table,\n",
    "#        ddl = meta_dict[table]['ddl'],\n",
    "#        load_cols = meta_dict[table]['load_cols'],\n",
    "#        load_func = meta_dict[table]['load_func'],\n",
    "#        db_dict = db_dict\n",
    "#    ).drop() # change func here\n",
    "#    \n",
    "#except KeyError: \n",
    "#    \n",
    "#    print(\"KeyError: check table var first. It must match a key in meta_dict\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All tables are now loaded...\n",
    "\n",
    "...and if, for any reason, we need to change them in any way, what we've built above should facilitate that. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
